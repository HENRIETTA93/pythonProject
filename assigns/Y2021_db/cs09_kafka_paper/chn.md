有大量的“日志”数据生成在任何规模
互联网公司。此数据通常包括 (1) 用户活动
与登录、浏览量、点击、“喜欢”相对应的事件，
分享、评论和搜索查询； (2) 服务调用栈、调用延迟、错误等操作指标，以及每台机器上的CPU、内存、网络或磁盘利用率等系统指标。长期以来，日志数据一直是用于跟踪用户参与度、系统利用率和其他指标的分析的一个组成部分。
然而，互联网应用程序的最新趋势使活动数据成为直接用于站点功能的生产数据管道的一部分。这些用途包括 (1) 搜索相关性，(2) 可能由项目流行度或活动流中的共同出现驱动的推荐，(3) 广告定位和报告，以及 (4) 防止滥用行为的安全应用程序，例如垃圾邮件或未经授权的数据抓取，以及 (5) 聚合用户状态更新或操作以供其“朋友”或“联系人”阅读的新闻源功能。
这种生产、实时使用日志数据创造了新的
数据系统面临的挑战，因为它的数量是
量级大于“真实”数据。例如，搜索，
推荐和广告通常需要计算
细粒度的点击率，不仅为用户的每次点击生成日志记录，还为每个页面上数十个未被点击的项目生成日志记录。中国移动每天收集 5-8TB 的电话记录 [11]，Facebook 收集近 6TB 的各种用户活动事件 [12]。
许多处理此类数据的早期系统依赖于
从生产服务器上物理抓取日志文件以进行分析。近年来，已经建立了几个专门的分布式日志聚合器，包括 Facebook 的 Scribe [6]、Yahoo 的 Data
高速公路 [4] 和 Cloudera 的水槽 [3]。这些系统是
主要用于收集日志数据并将其加载到数据仓库或 Hadoop [8] 中以供离线使用。在
LinkedIn（一个社交网站），我们发现除了
传统的离线分析，我们需要支持大部分
上面提到的实时应用程序没有更多的延迟
比几秒钟。
我们为日志处理构建了一个新的消息系统，称为 Kafka [18]，它结合了传统日志的优点
聚合器和消息传递系统。一方面，Kafka 是分布式和可扩展的，并提供高吞吐量。另一方面，Kafka 提供了一个类似于消息系统的 API，允许应用程序实时消费日志事件。 Kafka 已经开源并在 LinkedIn 的生产环境中成功使用了 6 个多月。它极大地简化了我们的基础设施，因为我们可以利用一个软件来在线和离线消费所有类型的日志数据。
本文的其余部分安排如下。我们重温
传统消息系统和日志聚合器在第 2 节中。在第 3 节中，我们描述了 Kafka 的架构及其关键设计原则。我们在第 4 节中描述了我们在 LinkedIn 上的 Kafka 部署，在第 5 节中描述了 Kafka 的性能结果。我们在第 6 节中讨论了未来的工作并得出结论。
2. 相关工作
传统的企业消息系统 [1][7][15][17]
存在很长时间，经常作为一个事件发挥关键作用
用于处理异步数据流的总线。但是，有几个原因导致它们往往不适合日志处理。
首先，企业提供的功能不匹配
系统。这些系统通常专注于提供丰富的
交货保证。例如，IBM Websphere MQ [7] 有
允许应用程序以原子方式将消息插入多个队列的事务支持。 JMS [14] 规范允许在消费后确认每个单独的消息，可能是无序的。这种交付保证对于收集日志数据来说通常是多余的。例如，偶尔丢失一些网页浏览事件当然不是世界末日。这些不需要的功能往往会增加 API 和这些系统的底层实现的复杂性。
其次，许多系统并没有像它们的主要设计约束那样强烈关注吞吐量。例如，JMS 没有 API 来允许生产者显式地将多条消息批处理到一个
   单一请求。这意味着每条消息都需要完整的 TCP/IP 往返，这对于我们域的吞吐量要求是不可行的。第三，这些系统在分布式支持方面很弱。没有简单的方法可以在多台机器上分区和存储消息。最后，许多消息系统假设消息几乎立即被消费，因此未消费的消息队列总是相当小。如果允许消息累积，它们的性能会显着下降，例如数据仓库应用程序等离线消费者会定期进行大量加载而不是持续消费。
在过去的几年中，已经建立了许多专门的日志聚合器。 Facebook 使用名为 Scribe 的系统。每个前端机器都可以通过套接字将日志数据发送到一组 Scribe 机器。每台 Scribe 机器都会聚合日志条目并定期将它们转储到 HDFS [9] 或 NFS 设备。雅虎的数据高速公路项目也有类似的数据流。一组机器聚合来自客户端的事件并推出“分钟”文件，然后将这些文件添加到 HDFS。 Flume 是 Cloudera 开发的一个相对较新的日志聚合器。它支持可扩展的“管道”和“接收器”，并使流式日志数据非常灵活。它还具有更多集成的分布式支持。然而，这些系统中的大多数都是为离线使用日志数据而构建的，并且经常不必要地暴露实现细节（例如“分钟文件”）
给消费者。此外，它们中的大多数使用“推送”模型，其中代理将数据转发给消费者。在 LinkedIn，我们发现“拉”模型更适合我们的应用程序，因为每个消费者都可以以它可以承受的最大速率检索消息，并避免被推送速度超过其处理速度的消息淹没。拉动模型还可以轻松地回滚消费者，我们将在第 3.2 节末尾讨论这一好处。
最近，雅虎！研究开发了一种新的分布式
发布/订阅系统称为 HedWig [13]。 HedWig 具有高度可扩展性
可用，并提供强大的耐用性保证。但是，它主要用于存储数据存储的提交日志。
三、Kafka架构与设计原则
由于现有系统的限制，我们开发了一个新的基于消息的日志聚合器 Kafka。我们先介绍Kafka的基本概念。特定类型的消息流由主题定义。生产者可以向主题发布消息。
然后将发布的消息存储在一组称为代理的服务器中。消费者可以从代理订阅一个或多个主题，并通过从代理拉取数据来消费订阅的消息。
消息传递在概念上很简单，我们试图让 Kafka API 同样简单来反映这一点。我们没有展示确切的 API，而是展示了一些示例代码来展示如何使用 API。下面给出了生产者的示例代码。消息被定义为仅包含字节的有效负载。用户可以选择她最喜欢的序列化方法来对消息进行编码。为了提高效率，生产者可以在单个发布请求中发送一组消息。
   要订阅主题，消费者首先为该主题创建一个或多个消息流。发布到那个的消息
   主题将均匀分布到这些子流中。细节
关于 Kafka 如何分发消息将在后面描述
第 3.2 节。每个消息流提供一个迭代器接口
通过正在产生的连续消息流。这
消费者然后迭代流中的每条消息，
处理消息的有效负载。不同于传统的迭代器，
消息流迭代器永远不会终止。如果目前有
没有更多的消息要消费，迭代器阻塞直到新的
消息发布到主题。我们支持多个消费者联合的点对点交付模式
消费主题中所有消息的单个副本，以及
发布/订阅模型，其中每个消费者都有多个消费者
检索自己的主题副本。
Kafka的整体架构如图1所示。
Kafka 本质上是分布式的，一个 Kafka 集群通常由
多个经纪人。为了平衡负载，一个主题被分为
多个分区，每个代理存储其中一个或多个
分区。多个生产者和消费者可以发布和
同时检索消息。在第 3.1 节中，我们描述了
代理上单个分区的布局和一些设计选择
我们选择它来提高访问分区的效率。在节
3.2，我们描述了生产者和消费者如何交互
分布式设置中的多个代理。我们讨论交货
第 3.3 节中的 Kafka 保证。
3.1 单个分区的效率
我们在 Kafka 中做出了一些决定，以使系统高效。
简单的存储：Kafka 有一个非常简单的存储布局。每个
一个主题的分区对应一个逻辑日志。物理上，一个日志
被实现为一组大约为
相同的大小（例如，1GB）。生产者每次发布消息时
到一个分区，代理只需将消息附加到最后一个
段文件。为了获得更好的性能，我们将段文件刷新到
仅在已配置的消息数量之后磁盘
已发布或已过一定时间。一条消息是
仅在冲洗后才暴露给消费者
与典型的消息系统不同，存储在 Kafka 中的消息
没有明确的消息 ID。相反，每条消息都是
由其在日志中的逻辑偏移量寻址。这避免了开销
维护辅助的、查找密集的随机访问索引
将消息 ID 映射到实际消息的结构
地点。请注意，我们的消息 ID 正在增加，但没有增加
连续的。要计算下一条消息的 id，我们必须
将当前消息的长度添加到其 id 中。从今以后，我们
将交替使用消息 ID 和偏移量。
消费者总是消费来自特定的消息
依次分区。如果消费者承认特定
消息偏移，它意味着消费者已经收到了所有
分区中该偏移量之前的消息。在幕后，
消费者正在向代理发出异步拉取请求以
准备好数据缓冲区供应用程序使用。每个
拉取请求包含消息的偏移量
消费开始并获取可接受的字节数。
每个代理在内存中保存一个排序的偏移列表，包括
每个段文件中第一条消息的偏移量。经纪人
定位请求消息所在的段文件
搜索偏移量列表，并将数据发送回消费者。
消费者收到消息后，计算消息的偏移量
要消费的下一条消息并在下一个拉取请求中使用它。这
Kafka 日志和内存中索引的布局如图所示
图 2. 每个框显示消息的偏移量。
高效传输：我们在传输数据时非常小心
从卡夫卡出来。早些时候，我们已经证明了生产者可以
在单个发送请求中提交一组消息。虽然
终端消费者 API 一次迭代一条消息，在
涵盖，来自消费者的每个拉取请求也检索多个
消息达到特定大小，通常为数百 KB。
我们做出的另一个非常规选择是明确避免
在 Kafka 层的内存中缓存消息。相反，我们依靠
在底层文件系统页面缓存上。这主要有
避免双缓冲的好处---仅缓存消息
在页面缓存中。这具有保留的额外好处
即使代理进程重新启动，也可以进行热缓存。自从卡夫卡
根本不缓存正在处理的消息，它的开销很小
在垃圾收集其内存，使高效
以基于 VM 的语言实现是可行的。最后，由于
生产者和消费者都访问段文件
   顺序地，消费者经常落后于生产者
少量，正常的操作系统缓存启发式是
非常有效（特别是直写缓存和预读）。我们发现，无论是生产还是
消费具有与数据大小成线性关系的一致性能，
高达数 TB 的数据。
此外，我们优化了消费者的网络访问。卡夫卡
是一个多用户系统，一条消息可能是
被不同的消费者应用程序多次消费。一种
将字节从本地文件发送到远程文件的典型方法
socket 涉及以下步骤： (1) 从存储中读取数据
媒体到操作系统中的页面缓存，（2）复制页面缓存中的数据
到一个应用程序缓冲区，(3) 将应用程序缓冲区复制到另一个
内核缓冲区， (4) 将内核缓冲区发送到套接字。这个
包括4个数据复制和2个系统调用。在 Linux 和其他
Unix 操作系统，存在一个发送文件 API [5]，可以
直接将字节从文件通道传输到套接字通道。
这通常避免引入 2 个副本和 1 个系统调用
在步骤 (2) 和 (3) 中。 Kafka 利用 sendfile API 高效地
将日志段文件中的字节从代理传递给消费者。
无状态代理：与大多数其他消息传递系统不同，在
Kafka，每个消费者有多少的信息
消费的不是由经纪人维护，而是由消费者维护
本身。这样的设计降低了很多复杂性和
经纪人的开销。然而，这使得删除一个
消息，因为经纪人不知道是否所有订阅者
已经消耗了消息。 Kafka 通过使用一个
用于保留策略的简单的基于时间的 SLA。一条消息是
如果它在代理中保留的时间更长，则自动删除
超过一定期限，通常为 7 天。此解决方案适用于
实践。大多数消费者，包括线下消费者，都完成了
每天、每小时或实时消费。事实是
Kafka 的性能不会随着数据量的增加而降低
使这种长期保留成为可能。
这种设计有一个重要的附带好处。一个消费者可以
故意倒回到旧的偏移量并重新使用数据。
这违反了队列的共同契约，但被证明是一个
许多消费者的基本功能。例如，当有
消费者中的应用程序逻辑错误，应用程序可以
修复错误后重新播放某些消息。这是
对于将 ETL 数据加载到我们的数据仓库中尤其重要
或 Hadoop 系统。作为另一个例子，消费的数据可能
仅定期刷新到持久存储（例如，全文
索引器）。如果消费者崩溃，则未刷新的数据将丢失。在
在这种情况下，消费者可以检查点的最小偏移量
未刷新的消息并在该偏移量出现时重新使用
重新启动。我们注意到倒带消费者要容易得多
在拉模型中支持比推模型。
3.2 分布式协调
我们现在描述生产者和消费者的行为
分布式设置。每个生产者都可以向
随机选择的分区或语义上的分区
由分区键和分区函数决定。我们
将关注消费者如何与经纪人互动。
Kafka 有消费群体的概念。每个消费群体
由一个或多个共同消费一组的消费者组成
订阅的主题，即每条消息只传递给其中一个
组内的消费者。不同的消费群体
独立消费全套订阅的消息，没有
需要跨消费者群体进行协调。消费者
同一组内可以在不同的进程或不同的
机器。我们的目标是划分存储在代理中的消息
均匀分布在消费者中，不会引入太多
协调开销。
我们的第一个决定是使主题内的分区最小
并行度单位。这意味着在任何给定时间，所有
来自一个分区的消息只被一个
每个消费者组内的消费者。如果我们允许多个
消费者同时消费一个分区，他们
将不得不协调谁消费什么消息，哪个
需要锁定和状态维护开销。相比之下，
在我们的设计消费过程中只需要协调时
消费者重新平衡负载，这是一个罕见的事件。为了
要真正平衡负载，我们需要更多分区
主题比每个组中的消费者。我们可以轻松实现
这是通过对主题进行过度分区。
我们做出的第二个决定是没有一个中央
“主”节点，而是让消费者之间协调
自己以去中心化的方式。添加主可以
使系统复杂化，因为我们不得不进一步担心
主故障。为了便于协调，我们采用它应该使用的分区子集。过程是
在算法 1 中描述。通过读取代理和消费者
来自 Zookeeper 的注册表，消费者首先计算集合（PT）
每个订阅主题 T 和集合 (CT) 可用的分区数
订阅 T 的消费者。然后将 PT 范围划分为 |CT|
块并确定性地选择一个块来拥有。对于每个
消费者选择的分区，它将自己写为新的所有者
所有权注册表中的分区。最后，消费者
开始一个线程从每个拥有的分区中提取数据，开始
来自存储在偏移注册表中的偏移。随着消息的获取
从分区中提取，消费者定期更新
偏移注册表中最近消耗的偏移量。
当一个组中有多个消费者时，每个消费者
将收到有关经纪人或消费者更改的通知。然而
通知可能在消费者的时间略有不同。
因此，有可能一个消费者试图拥有一个
分区仍然归另一个消费者所有。当这种情况发生时，
第一个消费者简单地释放它当前的所有分区
拥有，稍等片刻，然后重试重新平衡过程。在实践中，
重新平衡过程通常在仅重试几次后就稳定下来。
当一个新的消费者组被创建时，没有偏移量可用
抵消注册表。在这种情况下，消费者将从
最小或最大偏移量（取决于
配置）在每个订阅的分区上可用，使用
我们在代理上提供的 API。
3.3 交货保证
一般来说，Kafka 只保证至少一次交付。 Exactly once 交付通常需要两阶段提交，而不是
我们的应用程序所必需的。大多数时候，一条消息是
只交付一次给每个消费群体。然而，在
消费者进程在没有干净关闭的情况下崩溃的情况，
接管那些拥有的分区的消费者进程
失败的消费者可能会收到一些重复的消息
在最后一个偏移量成功提交给zookeeper之后。如果
应用程序关心重复，它必须添加自己的重复数据删除逻辑，或者使用我们返回的偏移量
消费者或消息中的某个唯一键。这通常是
比使用两阶段提交更具成本效益的方法。
Kafka 保证来自单个分区的消息是
按顺序交付给消费者。但是，不能保证
关于来自不同分区的消息的排序。
   
为了避免日志损坏，Kafka 为每条消息存储了一个 CRC
日志。如果代理上有任何 I/O 错误，Kafka 会运行一个
恢复过程以删除那些不一致的消息
CRC。在消息级别拥有 CRC 还允许我们
在消息产生或消费后检查网络错误。
如果代理宕机，存储在其上的任何消息尚未消耗
变得不可用。如果代理上的存储系统是
永久损坏，任何未使用的消息将永远丢失。
未来，我们计划在 Kafka 中添加内置复制功能
将每条消息冗余存储在多个代理上。
4. LinkedIn 在 Kafka 的使用
在本节中，我们将描述我们如何在 LinkedIn 上使用 Kafka。数字
图 3 显示了我们部署的简化版本。我们有一个
Kafka 集群与我们面向用户的服务运行的每个数据中心位于同一地点。前端服务产生各种
并批量发布到本地的Kafka brokers。
我们依靠硬件负载平衡器来分发发布
向一组 Kafka 代理均匀发送请求。线上消费者
Kafka 在同一数据中心内的服务中运行。
我们还在单独的数据中心部署了一个 Kafka 集群，用于
离线分析，地理位置靠近我们的 Hadoop
集群和其他数据仓库基础设施。这个实例
Kafka 运行一组嵌入式消费者来从
实时数据中心中的 Kafka 实例。然后我们运行数据加载作业
将数据从这个 Kafka 副本集群中提取到 Hadoop 和我们的
数据仓库，我们在其中运行各种报告作业和
数据的分析过程。我们还使用这个 Kafka 集群
原型设计并能够针对
用于临时查询的原始事件流。无需过多调音，
完整管道的端到端延迟约为 10
平均秒，足以满足我们的要求。
目前，Kafka 积累了数百 GB 的数据和
每天接近 10 亿条消息，我们预计这一数字还会增长
当我们完成将遗留系统转换为
卡夫卡的优势。更多类型的消息将添加到
未来。重新平衡过程能够自动重定向
操作人员启动或停止代理时的消耗
软件或硬件维护。
我们的跟踪还包括一个审计系统，以验证有
整个管道没有数据丢失。为方便起见，每个
消息携带时间戳和服务器名称
生成。我们对每个生产者进行检测，以便定期
生成一个监控事件，它记录了
该生产者为固定范围内的每个主题发布的消息
时间窗口。生产者将监控事件发布到
Kafka 在一个单独的主题中。然后消费者可以计算
他们从给定主题收到的消息数
并使用要验证的监控事件来验证这些计数
数据的正确性。
加载到Hadoop集群是通过实现
一种特殊的 Kafka 输入格式，允许 MapReduce 作业
直接从Kafka读取数据。 MapReduce 作业加载原始数据
数据，然后对它进行分组和压缩以进行高效处理
未来。消息的无状态代理和客户端存储
偏移量在这里再次发挥作用，允许 MapReduce 任务
管理（允许任务失败并重新启动）以
以自然的方式处理数据加载而不会重复或丢失
任务重新启动时的消息。数据和偏移量都是
仅在作业成功完成后才存储在 HDFS 中。
我们选择使用 Avro [2] 作为我们的序列化协议，因为它是
高效并支持模式演变。对于每条消息，我们
将其 Avro 架构的 id 和序列化字节存储在
有效载荷。这种模式允许我们强制执行合同以确保
数据生产者和消费者之间的兼容性。我们使用一个
轻量级架构注册服务将架构 ID 映射到
实际架构。当消费者收到一条消息时，它会查找
用于检索架构的架构注册表，该架构用于
将字节解码为一个对象（这个查找只需要完成
每个模式一次，因为值是不可变的）。
5. 实验结果
我们进行了一项实验研究，比较了性能
Kafka 与 Apache ActiveMQ v5.4 [1]，一个流行的开源
JMS 的实现，以及 RabbitMQ v2.4 [16]，一条消息
系统以其性能而著称。我们使用了 ActiveMQ 的默认值
持久消息存储 KahaDB。虽然这里没有介绍，
我们还测试了一个替代的 AMQ 消息存储并发现它的
性能与 KahaDB 非常相似。只要有可能，
我们尝试在所有系统中使用可比较的设置。
我们在 2 台 Linux 机器上运行我们的实验，每台机器都有 8 2GHz
核心，16GB 内存，6 个 RAID 10 磁盘。两个
机器通过 1Gb 网络链接连接。其中一个
机器被用作经纪人
   

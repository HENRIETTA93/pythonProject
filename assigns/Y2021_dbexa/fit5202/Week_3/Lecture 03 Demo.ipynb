{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "master = \"local[3]\"\n",
    "app_name = \"Parallel Join Demo\"\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_R = [(8,\"Adele\"), \n",
    "           (22, \"Bob\"), \n",
    "           (16, \"Clement\"), \n",
    "           (23, \"Dave\"), \n",
    "           (11, \"Ed\"), \n",
    "           (25, \"Fung\"), \n",
    "           (3, \"Goel\"), \n",
    "           (17, \"Harry\"), \n",
    "           (14, \"Irene\"), \n",
    "           (2, \"Joanna\"), \n",
    "           (6, \"Kelly\"), \n",
    "           (20, \"Lim\"), \n",
    "           (1, \"Meng\"), \n",
    "           (5, \"Noor\"), \n",
    "           (19, \"Omar\")]\n",
    "table_S = [(8,\"Arts\"), \n",
    "           (15, \"Dance\"), \n",
    "           (10, \"Geology\"), \n",
    "           (12, \"Business\"), \n",
    "           (7, \"Engineering\"), \n",
    "           (11, \"Health\"), \n",
    "           (2, \"CompSc\"), \n",
    "           (21, \"Finance\"), \n",
    "           (18, \"IT\")]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R = spark.createDataFrame(table_R, ['Id', 'Name'])\n",
    "df_S = spark.createDataFrame(table_S, ['Id', 'Department'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+----------+\n",
      "| Id|  Name| Id|Department|\n",
      "+---+------+---+----------+\n",
      "|  8| Adele|  8|      Arts|\n",
      "| 11|    Ed| 11|    Health|\n",
      "|  2|Joanna|  2|    CompSc|\n",
      "+---+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined_sortmerge = df_R.join(df_S, df_R.Id == df_S.Id, how='inner')\n",
    "df_joined_sortmerge.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [Id#0L], [Id#4L], Inner\n",
      ":- *(2) Sort [Id#0L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(Id#0L, 200), true, [id=#76]\n",
      ":     +- *(1) Filter isnotnull(Id#0L)\n",
      ":        +- *(1) Scan ExistingRDD[Id#0L,Name#1]\n",
      "+- *(4) Sort [Id#4L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(Id#4L, 200), true, [id=#82]\n",
      "      +- *(3) Filter isnotnull(Id#4L)\n",
      "         +- *(3) Scan ExistingRDD[Id#4L,Department#5]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined_sortmerge.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Which __Parallel Join Algorithm__ is used by Apache Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Left Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----------+\n",
      "| Id|   Name|  Id|Department|\n",
      "+---+-------+----+----------+\n",
      "| 19|   Omar|null|      null|\n",
      "| 22|    Bob|null|      null|\n",
      "| 25|   Fung|null|      null|\n",
      "|  6|  Kelly|null|      null|\n",
      "| 17|  Harry|null|      null|\n",
      "|  5|   Noor|null|      null|\n",
      "|  1|   Meng|null|      null|\n",
      "|  3|   Goel|null|      null|\n",
      "|  8|  Adele|   8|      Arts|\n",
      "| 11|     Ed|  11|    Health|\n",
      "|  2| Joanna|   2|    CompSc|\n",
      "| 14|  Irene|null|      null|\n",
      "| 23|   Dave|null|      null|\n",
      "| 20|    Lim|null|      null|\n",
      "| 16|Clement|null|      null|\n",
      "+---+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left_outer_join = df_R.join(df_S, df_R.Id == df_S.Id, how='left')\n",
    "df_left_outer_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "SortMergeJoin [Id#0L], [Id#4L], LeftOuter\n",
      ":- *(2) Sort [Id#0L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(Id#0L, 200), true, [id=#156]\n",
      ":     +- *(1) Scan ExistingRDD[Id#0L,Name#1]\n",
      "+- *(4) Sort [Id#4L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(Id#4L, 200), true, [id=#161]\n",
      "      +- *(3) Filter isnotnull(Id#4L)\n",
      "         +- *(3) Scan ExistingRDD[Id#4L,Department#5]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left_outer_join.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Which of __Parallel Outer Join Processing Method__ is used by Apache Spark?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
